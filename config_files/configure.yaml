BasicSettings:
  ImageSize: 64
  ImageChannel: 3
  ReplayBufferOnGPU: True
  Seed: 3710
  Env_name: ALE/MontezumaRevenge-v5
  Device: cuda:0
  Use_amp: True
  Use_cg: True
  Compile: True
  SavePath: None

  CausalGradCheckpoint: True    # Saves 40% VRAM
  CausalChunkSize: 8            # For long sequences

Evaluate:
   EpisodeNum: 4
   NumEnvs: 4
   DuringTraining: False
   EverySteps: 1000

JointTrainAgent:
  SampleMaxSteps: 200000 # Just to make sure the last episode will finish, no training after 100k
  BufferMaxLength: 100000
  WorldModelWarmUp: 1032
  BehaviourWarmUp: 1032
  NumEnvs: 1
  BatchSize: 16
  BatchLength: 128
  ImagineBatchSize: 1024
  ImagineContextLength: 8
  ImagineBatchLength: 16
  RealityContextLength: 16
  TrainDynamicsEverySteps: 1
  TrainDynamicsEpoch: 1
  TrainAgentEverySteps: 1
  FreezeWorldModelAfterSteps: 100000
  FreezeBehaviourAfterSteps: 100000
  SaveEverySteps: 2000
  SaveModels: True
  Tau: 10
  ImaginationTau: 10
  Alpha: 1.0 # High focus on penalising high imagine counts regardless of train counts, less probability to be sampled
  Beta: 1.0 # High focus on penalising

  CausalWarmUp: 2000              # Causal model pretraining
  AlternateTrainEvery: 5          # Alternate WM/causal training

Models:
  UseCausal: True
  WorldModel:
    dtype: float32
    Backbone: Mamba2 # Mamba, Mamba2, Transformer
    InChannels: 3
    Act: SiLU
    CategoricalDim: 32
    ClassDim: 32
    HiddenStateDim: 512
    Optimiser: Laprop
    LatentDiscreteType: naive
    Max_grad_norm: 1000
    Warmup_steps: 1000
    Dropout: 0.1
    Unimix_ratio: 0.01
    Weight_decay: 1.0e-4
    Adam:
      LearningRate: 1.0e-4
    Laprop:
      LearningRate: 4.0e-5
      Epsilon: 1.0e-20
    Encoder:
      Depth: 16
      Mults:  [1, 2, 3, 4, 4]
      Norm: rms
      Kernel: 5
      Padding: same
      InputSize: [3, 64, 64]
    Decoder:
      Depth: 16
      Mults:  [1, 2, 3, 4, 4]
      Norm: rms
      Kernel: 5
      Padding: same
      FirstStrideOne: True
      InputSize: [3, 64, 64]
      FinalLayerSigmoid: True
    Reward:
      HiddenUnits: 256
      LayerNum: 1
    Termination:
      HiddenUnits: 256
      LayerNum: 1
    Transformer:
      FinalFeatureWidth: 4
      NumLayers: 2
      NumHeads: 8
    Mamba:
      n_layer: 2
      d_intermediate: 0
      ssm_cfg:
        d_state: 16

  CausalModel:
    dtype: float32
    TrCodeDim: 256               # [16, 24, 32, 64] Dimension of codebook entries
    ReCodeDim: 128
    NumCodesTr: 128           # Transition codebook size
    NumCodesRe: 64            # Reward codebook size
    HiddenDim: 512           # Hidden dimension for MLPs
    UseConfounder: False      # Flag to use confounder encoder and approximator

    Encoder:                  # Avoid bottlenecking -> hidden_dim == hidden_state_dim
      TransProjDim: 256       # Dim of projection of h into transition continuous embeddings
      RewProjDim: 128         # Dim of projection of h into reward continuous embeddings
      HiddenDim: 512          # Hidden layer dimensions
      Activation: SiLU        # Activation function

    Quantizer:
      TransitionTemp: 1.0     # Initial Gumbel softmax temp for transition codebook
      RewardTemp: 1.0         # Initial Gumbel softmax temp for transition codebook
      TransitionMinTemperature: 0.1     # Lower temperature bound for transition codebook
      RewardMinTemperature: 0.1     # Lower temperature bound for reward codebook
      TransitionAnnealFactor: 0.97      # Temp decay per update for transition codebook
      RewardAnnealFactor: 0.95          # Temp decay per update for reward codebook
      BetaTransition: 0.25              # Commitment loss weight for transitions
      BetaReward: 0.25                  # Commitment loss weight for rewards
      NormalizedInputs: True  # L2 normalize inputs/codebook
      Coupling: False          # Enable codebook coupling
      SparsityWeight: 0.1     # Sparsity Loss for coupling
      LambdaCouple: 0.1       # Coupling loss weight
      UseCDist: False

    Confounder:
      ConfDim: 128            # Confounder Dimensions
      HiddenDim: 512          # Hidden layer dimensions
      PriorLayers: 2          # MLP layers for prior network
      PostLayers: 2           # MLP layers for posterior
      AffineInitStd: 0.01     # Affine param initialization
      MinLogVar: -6.0         # Posterior variance bounds
      MaxLogVar: 0.0
      PriorMomentum: 0.97
      PriorRegWeight: 0.01           # Regularization loss weight
      PriorCodeAlign: 0.1       # Weight for code alignment loss
      PostReg: 0.001          # Posterior L2 regularization
      PostCodeSparsity: 0.01  #
      PostKLWeight: 0.5

    Modulation:
      MaskInitScale: 0.5      # Initial scale for modulation
      MaskSparsity: 0.8       # Target sparsity ratio
      MinScale: 0.1           # Clamp scale values
      MaxScale: 1.9           # Upper clamp scale value

    Predictors:
      ComputeInvarianceLoss: True # Enforces invariance in codes
      InvarianceWeight: 0.3     # Mechanism invariance loss
      TransitionWeight: 1.0 # Main objective - next state prediction
      RewardWeight: 1.0         # Main objective - reward prediction
      TerminationWeight: 0.5    # Secondary Objective - episode termination
      Transition:
        UseImportanceWeightedMoE: True
        NumOfExperts: 8
        TopK: 3
        HiddenDim: 512
        MaskSparsityWeight: 0.05
        AuxiliaryWeight: 0.1

      Reward:
        HiddenDim: 512
        NumHeads: 4

      Termination:
        HiddenDim: 512
        Activation: SiLU
        NumLayers: 2
        Dropout: 0.1

    Integration:
      GradDetachWorld: True   # Detach world model gradients
      AnnealSteps: 1000       # Mask sparsity warmup

  Agent:
    dtype: float32
    Policy: AC # AC or PPO
    Unimix_ratio: 0
    AC:
      NumLayers: 3
      Gamma: 0.985
      Lambda: 0.95
      EntropyCoef: 3.e-4
      Max_grad_norm: 100
      Warmup_steps: 1000
      Act: SiLU
      Optimiser: Laprop
      Adam:
        LearningRate: 3.0e-5
        Epsilon: 1.0e-5
      Laprop:
        LearningRate: 4.0e-5
        Epsilon: 1.0e-20
      Actor:
        HiddenUnits: 256
      Critic:
        HiddenUnits: 512
    PPO:
      NumLayers: 3
      Gamma: 0.985
      Lambda: 0.95
      EpsilonClip: 0.2
      K_epochs: 3
      Minibatch: 16384
      CriticCoef: 1
      EntropyCoef: 3.e-4
      KL_threshold: 0.01
      Max_grad_norm: 100
      Warmup_steps: 1000
      Act: SiLU
      Optimiser: Laprop
      Adam:
        LearningRate: 3.e-5
        Epsilon: 1.0e-5
      Laprop:
        LearningRate: 4.0e-5
        Epsilon: 1.0e-20
      Actor:
        HiddenUnits: 256
      Critic:
        HiddenUnits: 512 # Andrychowicz2020 wider critic network seems better
Wandb:
  Init:
    Mode: disabled
    Project: Causal_Mamba
n: standard